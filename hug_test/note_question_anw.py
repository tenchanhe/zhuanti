from transformers import TapexTokenizer, BartForConditionalGeneration
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import pandas as pd
import timeit


tokenizer = AutoTokenizer.from_pretrained("neulab/omnitab-large-finetuned-wtq")
model = AutoModelForSeq2SeqLM.from_pretrained("neulab/omnitab-large-finetuned-wtq")
# tokenizer = AutoTokenizer.from_pretrained("google/tapas-base-finetuned-sqa")
# model = AutoModelForSeq2SeqLM.from_pretrained("google/tapas-base-finetuned-sqa")


start = timeit.default_timer()

# data = {
    # "year": [1896, 1900, 1904, 2004, 2008, 2012],
    # "city": ["athens", "paris", "st. louis", "athens", "beijing", "london"]
# }
# data = {
    # "teacher": ['彭朱如','蔡維奇、羅明琇','成力庚、黃家齊','傅浚映','黃韋仁','郭曉玲','吳恬妤、黃家齊','張彥成','謝凱宇','翁玲華'],
    # "difficult": ['0.818219543','0.667401671','0.333327234','0.929928958','0.913782001','0.844378352','0.892524183','0.936422229','0.529311478','0.881143749'],
    # "busy":['0.802474141','0.551495731','0.127664044','0.955125213','0.962145984','0.918035507','0.976785362','0.973572195','0.545516372','0.921947479']
# }
data = {
    "teacher_name":['彭朱如','蔡維奇、羅明琇','成力庚、黃家齊','傅浚映','黃韋仁','郭曉玲','吳恬妤、黃家齊','張彥成','謝凱宇','翁玲華','胡昌亞','李瓊淑','曹莉玲','廖經維','胡昌亞、范思美、陳燕諭','林呈欣','林介勝','胡昌亞、范思美',],
    "difficult":['0.8182195425033569','0.6674016714096069','0.3333272337913513','0.9299289584159852','0.913782000541687','0.8443783521652222','0.8925241827964783','0.9364222288131714','0.52931147813797','0.8811437487602234','0.7341459393501282','0.9752620458602904','0.5069355964660645','0.745098888874054','0.6783252954483032','0.6532494425773621','0.8181560635566711','0.8549715280532837'],
    "busy":['0.8024741411209106','0.5514957308769226','0.1276640444993972','0.9551252126693726','0.962145984172821','0.9180355072021484','0.9767853617668152','0.9735721945762634','0.5455163717269897','0.9219474792480468','0.6329165101051331','0.9871159195899964','0.4055183529853821','0.8402018547058105','0.2668387293815613','0.6812968850135803','0.935681164264679','0.8130151033401489'],
    "low":['0.9665459394454956','0.6267873048782349','0.5527256727218628','0.827985405921936','0.8523410558700562','0.5837082862854004','0.736656665802002','0.8222642540931702','0.2694483399391174','0.6275872588157654','0.1288109868764877','0.952942669391632','0.2892338931560516','0.572537362575531','0.2642002999782562','0.4599970579147339','0.7913257479667664','0.545236349105835'],
    "high":['0.482124924659729','0.5002021789550781','0.1042820140719413','0.9572854042053224','0.97504985332489','0.8847326040267944','0.9523383975028992','0.9857455492019652','0.6457886695861816','0.9188355207443236','0.822962760925293','0.9933499693870544','0.701541543006897','0.5815527439117432','0.8770330548286438','0.7774893045425415','0.4039492905139923','0.8211548924446106'],
    "easy":['0.0672395527362823','0.3651922047138214','0.0070264772512018','0.9292572140693665','0.7526476383209229','0.625724196434021','0.9011847376823424','0.9498264193534852','0.2009154260158538','0.7699266672134399','0.2319972813129425','0.9769645929336548','0.1511406153440475','0.5404032468795776','0.0326444618403911','0.1479864418506622','0.3873880505561828','0.1098375767469406'],
    "interesting":['0.9903184175491332','0.981223464012146','0.52016681432724','0.9874096512794496','0.9839252233505248','0.9565125107765198','0.9821392297744752','0.9915090203285216','0.7232750058174133','0.9335325360298156','0.9969027042388916','0.9947583079338074','0.9908666610717772','0.8607321381568909','0.9884963631629944','0.9869756102561952','0.9401026368141174','0.9426320791244508'],
    "clear":['0.5473873019218445','0.5911794900894165','0.1804862320423126','0.9907385110855104','0.98188453912735','0.9486432075500488','0.9853790402412416','0.9827065467834472','0.6707909107208252','0.9516799449920654','0.99438738822937','0.9944059252738952','0.8863174319267273','0.6994434595108032','0.3871765732765198','0.597894012928009','0.7494900226593018','0.9424507021903992'],
    "implicit":['0.6525604128837585','0.7784960269927979','0.0923106893897056','0.9551089406013488','0.9722049832344056','0.9235035181045532','0.9417707920074464','0.9792248606681824','0.6241302490234375','0.9506616592407228','0.7410716414451599','0.9863289594650269','0.5244704484939575','0.827953577041626','0.5588725805282593','0.7049762010574341','0.8576213121414185','0.702308714389801'],
    "heavy":['0.6426762938499451','0.2342578470706939','0.0243015848100185','0.9554015398025512','0.926597535610199','0.8451985716819763','0.9457038640975952','0.978213667869568','0.4932425916194916','0.8612047433853149','0.4210889935493469','0.989604651927948','0.274267166852951','0.6440996527671814','0.1957361102104187','0.1224479973316192','0.8022432923316956','0.4243305325508117'],
    "boring":['0.0547147579491138','0.0360186137259006','0.1139731481671333','0.6399707794189453','0.6549503803253174','0.4175378084182739','0.6667240858078003','0.852242112159729','0.0894062146544456','0.783369243144989','0.0071602626703679','0.9112622141838074','0.0437767915427684','0.311459332704544','0.0012067810166627','0.0069695636630058','0.2986179292201996','0.0047380193136632'],
    "thoughtful":['0.962038278579712','0.904721975326538','0.6083024144172668','0.9879163503646852','0.9839964509010316','0.9629449844360352','0.9903594851493835','0.9898366928100586','0.7861592769622803','0.95767080783844','0.978513479232788','0.9944120049476624','0.9956875443458556','0.8504574298858643','0.9682583808898926','0.954800546169281','0.8905423283576965','0.9794318675994872'],
    "sweet":['0.8865259885787964','0.7870907783508301','0.7899476289749146','0.985966682434082','0.9757654070854188','0.914250373840332','0.9546411037445068','0.9881240725517272','0.4797501564025879','0.9343036413192748','0.9818061590194702','0.9938768744468688','0.9524383544921876','0.7497192025184631','0.8709351420402527','0.89239501953125','0.8998939990997314','0.9941754937171936'],
    "hard":['0.6765177249908447','0.585900068283081','0.3443607687950134','0.9669559001922609','0.8967925310134888','0.8586413860321045','0.9329888224601746','0.9669472575187684','0.4984385371208191','0.9068599939346312','0.5911455750465393','0.974579930305481','0.4421788156032562','0.707518994808197','0.4446803629398346','0.5253283381462097','0.7713326215744019','0.7936821579933167'],
    "cold":['0.7488244771957397','0.1118908673524856','0.1484593898057937','0.8958470821380615','0.8826515078544617','0.6401506066322327','0.7859447002410889','0.9018462896347046','0.2665105164051056','0.8509117960929871','0.0442557856440544','0.9659549593925476','0.0253407340496778','0.5700528621673584','0.0050582052208483','0.0931118726730346','0.9898832440376282','0.1599714159965515']
}
# print(len(data['hard']))
table = pd.DataFrame.from_dict(data)

# query = "In which year did paris host the Olympic Games?"
# query = "Which city host the latest Olympic Games?"

query = "Which teachers course most difficult?"
# query = "Which teachers course most busy?"
print('hi')
encoding = tokenizer(table=table, query=query, return_tensors="pt")

end = timeit.default_timer()

outputs = model.generate(**encoding, max_new_tokens=3000)

print(query)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
# [' 2008']

